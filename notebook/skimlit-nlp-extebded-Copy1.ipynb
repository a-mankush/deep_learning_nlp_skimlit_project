{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:16:42.528437Z",
     "iopub.status.busy": "2024-02-24T17:16:42.527979Z",
     "iopub.status.idle": "2024-02-24T17:16:54.677295Z",
     "shell.execute_reply": "2024-02-24T17:16:54.676299Z",
     "shell.execute_reply.started": "2024-02-24T17:16:42.528401Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "\n",
    "import os\n",
    "for filename in os.listdir(\"pubmed-rct\"):\n",
    "    print(filename)\n",
    "\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "\n",
    "! ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\n",
    "\n",
    "filenames = [data_dir+file for file in os.listdir(data_dir)]\n",
    "filenames\n",
    "\n",
    "def get_lines(filename: str) -> list[str]:\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "  Takes in filename, reads its contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentence,\n",
    "  how many sentences are in the current abstract and what sentence number\n",
    "  the target line is.\n",
    "\n",
    "  Args:\n",
    "      filename: a string of the target text file to read and extract line data\n",
    "      from.\n",
    "\n",
    "  Returns:\n",
    "      A list of dictionaries each containing a line from an abstract,\n",
    "      the lines label, the lines position in the abstract and the total number\n",
    "      of lines in the abstract where the line is from. For example:\n",
    "\n",
    "      [{\"target\": 'CONCLUSION',\n",
    "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "        \"line_number\": 8,\n",
    "        \"total_lines\": 8}]\n",
    "  \"\"\"\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_sample = []\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "               line_data = {}\n",
    "               target_text_split = abstract_line.split(\"\\t\")\n",
    "               line_data[\"target\"] = target_text_split[0]\n",
    "               line_data[\"text\"] = target_text_split[1]\n",
    "               line_data[\"line_number\"] = abstract_line_number\n",
    "               line_data[\"total_lines\"] = len(abstract_line_split)\n",
    "               abstract_sample.append(line_data)\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_sample\n",
    "\n",
    "train_sample = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "test_sample = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "val_sample = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
    "\n",
    "len(train_sample), len(test_sample), len(val_sample)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.DataFrame(train_sample)\n",
    "test_df = pd.DataFrame(test_sample)\n",
    "val_df = pd.DataFrame(val_sample)\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "train_sentences = train_df.text.tolist()\n",
    "test_sentences = test_df.text.tolist()\n",
    "val_sentences = val_df.text.tolist()\n",
    "len(train_sentences), len(test_sentences), len(val_sample)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "train_labels_ohe = ohe.fit_transform(train_df.target.to_numpy().reshape(-1,1))\n",
    "val_labels_ohe = ohe.transform(val_df.target.to_numpy().reshape(-1,1))\n",
    "test_labels_ohe = ohe.transform(test_df.target.to_numpy().reshape(-1,1))\n",
    "\n",
    "train_labels_ohe, val_labels_ohe\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_label_encoded = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
    "test_label_encoded = label_encoder.transform(test_df['target'].to_numpy())\n",
    "val_label_encoded = label_encoder.transform(val_df['target'].to_numpy())\n",
    "\n",
    "train_label_encoded\n",
    "\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, TextVectorization, Conv1D, Input, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "max_tokens = 680000\n",
    "output_seq_len = 55\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=output_seq_len,\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "text_vocab = text_vectorizer.get_vocabulary()\n",
    "embedding = Embedding(\n",
    "    input_dim=len(text_vocab),# lenght of our vocab\n",
    "    output_dim=256,\n",
    "    mask_zero=True,\n",
    "    name='token_embedding'\n",
    ")\n",
    "\n",
    "def split_chars(text):\n",
    "    \"\"\"\n",
    "    Split sentence into characters\n",
    "    \"\"\"\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and [UNK]\n",
    "char_vectorizer = TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=int(290),\n",
    "    name='char_vectorizer'\n",
    ")\n",
    "\n",
    "char_vectorizer.adapt(train_chars)\n",
    "\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "char_embeding = Embedding(input_dim=len(char_vocab),\n",
    "                         output_dim=25,\n",
    "                         mask_zero=True,\n",
    "                         name='char_embed')\n",
    "\n",
    "\n",
    "\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df.line_number.to_numpy(), depth=16)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df.line_number.to_numpy(), depth=16)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df.line_number.to_numpy(), depth=16)\n",
    "\n",
    "\n",
    "train_total_lines_one_hot = tf.one_hot(train_df.total_lines.to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df.total_lines.to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df.total_lines.to_numpy(), depth=20)\n",
    "\n",
    "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars, train_line_numbers_one_hot, train_total_lines_one_hot))\n",
    "train_char_token_pos_label = tf.data.Dataset.from_tensor_slices(train_labels_ohe)\n",
    "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_label)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_char_token_pos_data\n",
    "\n",
    "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars, val_line_numbers_one_hot, val_total_lines_one_hot))\n",
    "val_char_token_pos_label = tf.data.Dataset.from_tensor_slices(val_labels_ohe)\n",
    "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_label)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_token_pos_data\n",
    "\n",
    "test_char_token_pos_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars, test_line_numbers_one_hot, test_total_lines_one_hot))\n",
    "test_char_token_pos_label = tf.data.Dataset.from_tensor_slices(test_labels_ohe)\n",
    "test_char_token_pos_dataset = tf.data.Dataset.zip((test_char_token_pos_data, test_char_token_pos_label)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_token_pos_data\n",
    "\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "from helper_functions import calculate_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "# Assuming your target labels are in a NumPy array named 'y_train'\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                     classes=np.unique(train_label_encoded),\n",
    "                                                     y=train_label_encoded)\n",
    "print(class_weights)  # Optional: View the calculated weights\n",
    "class_weights_dict = {}\n",
    "for i, weight in enumerate(class_weights):\n",
    "    class_weights_dict[i] = weight\n",
    "    \n",
    "class_weights_dict    \n",
    "\n",
    "# 1. Token inputs/Model\n",
    "token_input = keras.Input(shape=(1,), dtype=\"string\", name='token_input')\n",
    "text_vectorizer_layer = text_vectorizer(token_input)\n",
    "text_embedding = embedding(text_vectorizer_layer)\n",
    "bi_lstm_layer = keras.layers.Bidirectional(keras.layers.LSTM(64))(text_embedding)\n",
    "custom_token_model = keras.Model(inputs=token_input, outputs=bi_lstm_layer)\n",
    "\n",
    "# 2. Char Inputs/Model\n",
    "char_input = keras.Input(shape=(1,), dtype=\"string\", name='char_input')\n",
    "char_vectorizer_layer = char_vectorizer(char_input)\n",
    "char_embeding_layer = char_embeding(char_vectorizer_layer)\n",
    "bi_char_lstm = keras.layers.Bidirectional(keras.layers.LSTM(64))(char_embeding_layer)\n",
    "char_model = keras.Model(char_input, bi_char_lstm)\n",
    "\n",
    "# 2.1 Concat char and token layers\n",
    "char_token_embedding = keras.layers.Concatenate(name=\"char_token_embedding\")([custom_token_model.output,\n",
    "                                                                              char_model.output])\n",
    "z = layers.Reshape((1, 256))(char_token_embedding)  # Reshape to (1, 512) to fit into LSTM\n",
    "z = keras.layers.Bidirectional(keras.layers.LSTM(64))(z)\n",
    "z = layers.Dropout(0.5)(z) \n",
    "\n",
    "# 3. line_number inputs/Model\n",
    "line_number_input = keras.Input(shape=(16,), name=\"line_number_input\")\n",
    "line_number_output = keras.layers.Dense(128, activation='relu')(line_number_input)\n",
    "line_number_model = keras.Model(line_number_input, line_number_output)\n",
    "\n",
    "# 4. Total_line inputs/model\n",
    "total_line_input = keras.Input(shape=(20,), name='total_line_input')\n",
    "total_line_output = keras.layers.Dense(128, activation=\"relu\")(total_line_input)\n",
    "total_line_model = keras.Model(total_line_input, total_line_output)\n",
    "\n",
    "# 5. Concat the above layers \n",
    "concat_layer = keras.layers.Concatenate(name=\"concat_of_token_char_total_no\")([line_number_model.output,\n",
    "                                                                              total_line_model.output,\n",
    "                                                                              z])\n",
    "\n",
    "# 6. add dropout\n",
    "hidden_layer = keras.layers.Dense(256, activation='relu')(concat_layer)\n",
    "dropout_layer_2 = keras.layers.Dropout(0.5)(hidden_layer)\n",
    "outputs = keras.layers.Dense(5, activation='softmax')(dropout_layer_2)\n",
    "\n",
    "model_8b = keras.Model(inputs=[token_input,\n",
    "                             char_input,\n",
    "                             line_number_input,\n",
    "                             total_line_input], \n",
    "                     outputs=outputs)\n",
    "model_8b.summary()\n",
    "\n",
    "model_8b.compile(loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "                             optimizer=keras.optimizers.Adam(),\n",
    "                             metrics=['accuracy'])\n",
    "\n",
    "early_stoping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True, verbose=1)\n",
    "history_model_8b = model_8b.fit(train_char_token_pos_dataset,\n",
    "                             epochs=50,\n",
    "                             steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
    "                             validation_data=val_char_token_pos_dataset,\n",
    "                             validation_steps=int(0.1 * len(val_char_token_pos_dataset)),\n",
    "                             callbacks=[early_stoping],\n",
    "#                              class_weight=class_weights_dict\n",
    "                               )\n",
    "                                \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred_prob_model_8b = model_8b.predict(val_char_token_pos_dataset)\n",
    "print(pred_prob_model_8b[:5])\n",
    "pred_model_8b = tf.argmax(pred_prob_model_8b, axis=1)\n",
    "print(pred_model_8b[:5])\n",
    "results_model_8b = calculate_results(val_label_encoded, pred_model_8b)\n",
    "print(results_model_8b)\n",
    "print(classification_report(val_label_encoded, pred_model_8b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:22:27.533773Z",
     "iopub.status.busy": "2024-02-24T17:22:27.532954Z",
     "iopub.status.idle": "2024-02-24T17:23:25.589117Z",
     "shell.execute_reply": "2024-02-24T17:23:25.587850Z",
     "shell.execute_reply.started": "2024-02-24T17:22:27.533739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model_8b_char_customtoken_line_total/ (stored 0%)\n",
      "  adding: model_8b_char_customtoken_line_total/saved_model.pb (deflated 87%)\n",
      "  adding: model_8b_char_customtoken_line_total/variables/ (stored 0%)\n",
      "  adding: model_8b_char_customtoken_line_total/variables/variables.data-00000-of-00001 (deflated 24%)\n",
      "  adding: model_8b_char_customtoken_line_total/variables/variables.index (deflated 70%)\n",
      "  adding: model_8b_char_customtoken_line_total/fingerprint.pb (stored 0%)\n",
      "  adding: model_8b_char_customtoken_line_total/keras_metadata.pb (deflated 93%)\n",
      "  adding: model_8b_char_customtoken_line_total/assets/ (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "model_8b.save(\"model_8b_char_customtoken_line_total\")\n",
    "!zip -r file.zip model_8b_char_customtoken_line_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:23:25.592066Z",
     "iopub.status.busy": "2024-02-24T17:23:25.591717Z",
     "iopub.status.idle": "2024-02-24T17:23:26.632967Z",
     "shell.execute_reply": "2024-02-24T17:23:26.632019Z",
     "shell.execute_reply.started": "2024-02-24T17:23:25.592034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__  helper_functions.py\t\t   pubmed-rct\n",
      "file.zip     model_8b_char_customtoken_line_total\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:23:58.781215Z",
     "iopub.status.busy": "2024-02-24T17:23:58.780766Z",
     "iopub.status.idle": "2024-02-24T17:23:58.789441Z",
     "shell.execute_reply": "2024-02-24T17:23:58.788470Z",
     "shell.execute_reply.started": "2024-02-24T17:23:58.781182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='file.zip' target='_blank'>file.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/file.zip"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:24:35.013362Z",
     "iopub.status.busy": "2024-02-24T17:24:35.012669Z",
     "iopub.status.idle": "2024-02-24T17:24:58.607185Z",
     "shell.execute_reply": "2024-02-24T17:24:58.606338Z",
     "shell.execute_reply.started": "2024-02-24T17:24:35.013328Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_8b = keras.models.load_model(\"model_8b_char_customtoken_line_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-24T17:25:57.439596Z",
     "iopub.status.busy": "2024-02-24T17:25:57.439169Z",
     "iopub.status.idle": "2024-02-24T17:26:15.419412Z",
     "shell.execute_reply": "2024-02-24T17:26:15.418572Z",
     "shell.execute_reply.started": "2024-02-24T17:25:57.439562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 18s 13ms/step - loss: 0.8386 - accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.838618278503418, 0.8857738375663757]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_8b.evaluate(val_char_token_pos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelInstanceId": 1899,
     "sourceId": 2622,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2180,
     "sourceId": 2938,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
